# ruff: noqa: E501, RUF001, W291
from collections.abc import Sequence

from conc_lessons.utils.ansi import (
    ac,
    acb,
    bi,
    cya,
    df,
    dii,
    hea,
    hi,
    hiy,
    i,
    pc,
    pcb,
    pin,
    pur,
    red,
    res,
    star,
    tc,
    tcb,
    yel,
)

teory_intro = f"""\
{hea}â¯â¯ ConcorrÃªncia com Python {dii}â²muita teoria importanteâ³{res}
__L__
Essa aula serÃ¡ {hi} 99,99% {df} teÃ³rica. Vamos entender {bi}todas as partes{df} envolvidas 
em {ac}Concurrent Programming{df} com Python. {dii}(nÃ£o Ã© um tÃ³pico muito simples){df}
__L__
Esses sÃ£o {bi}OS ASSUNTOS{df} que vÃ£o definir se vocÃª deve usar {acb} asyncio {df}, {tcb} threads {df},
{pcb} processos {df} ou outro tipo de concorrÃªncia.{df} 
__L__
Exemplos do que vamos entender:
__L__
{star} Como o seu cÃ³digo Python chega na CPU__L__
{star} Os personagens (processo, thread, SO, CPython)__L__
{star} O que Ã© um processo e suas threads __L__
{star} OperaÃ§Ãµes {acb} I/O-bound {df} e {pcb} CPU-bound {df} __L__
{star} Multitarefa Cooperativa (asyncio) e Multitarefa preemptiva (SO) __L__
{star} Asyncio e Threading no Python __L__
{star} GIL {red}â™¥ {df} {dii}(Global Interpreter Lock){df} __L__
{star} Paralelismo com Multiprocessing, Threads e Interpreters no Python __L__
{star} e muitos outros relacionados...
__L__
EntÃ£o... Como uma visÃ£o geral, vamos comeÃ§ar conhecendo os personagens: 
{pur}ConcorrÃªncia{df}, {pc}Processo{df}, {tc}Thread{df}, {cya}Python{df} e o {ac}Sistema Operacional{df}. 
"""

teory_non_concurrent_python = f"""\
{hea}â¯â¯ Python com e sem concorrÃªncia{df}
__L__
{bi}Em condiÃ§Ãµes normais (sem processos ou threads adicionais), o python executa em 
um Ãºnico {hi} processo {df}, como uma Ãºnica {hi} thread {df} (Main Thread), em um 
Ãºnico nÃºcleo da CPU. 
__L__
Isso significa que o cÃ³digo Ã© {bi}executado de forma sequencial{df}, linha apÃ³s linha. 
__L__
Quando uma operaÃ§Ã£o demora, ela atrasa todas as outras que vierem depois. 
__L__
Ã‰ possÃ­vel executar mais de uma operaÃ§Ã£o de forma concorrente, seja de forma assÃ­ncrona, 
em threads ou em processos separados.
Este conceito Ã© chamado de {hi} ConcorrÃªncia {df} {dii}(Concurrent Programming){df}.
__L__
{hi} ConcorrÃªncia {df} Ã© a habilidade de um sistema em executar tarefas de forma de 
forma que possam progredir juntas para atingir um objetivo. Isso pode acontecer em 
tempo sobreposto ou em paralelo (quando a soluÃ§Ã£o envolve mÃºltiplos nÃºcleos da CPU).
__L__
{hi} ConcorrÃªncia {df} ğŸƒâ€â™€ï¸ Ã© um conceito amplo que envolve muitos padrÃµes e termos, como 
assincronismo, paralelismo e multitarefas.
__L__
Em {hi} Python {df}, podemos aplicar concorrÃªncia com os mÃ³dulos {acb} asyncio {df}, 
{tcb} threading {df}, {pcb} multiprocessing {df}, {hi} concurrent {df} e outros.
"""

teory_char_process = f"""\
{hea}â¯â¯ Processo (o contÃªiner){df}
__L__
{bi}Um processo representa um programa em execuÃ§Ã£o no sistema operacional.{df}
__L__
Ã‰ nele que ficam os {hi} recursos que o programa usa {df}, como o espaÃ§o de memÃ³ria, 
descritores de arquivos, variÃ¡veis globais, call stack e as {tcb} threads {df}.
__L__
Um processo Ã© isolado de outros processos. Isso Ã© importante para seguranÃ§a e 
vÃ¡rios outros fatores. Mas isso tambÃ©m dificulta a nossa vida ğŸ¤£.
__L__
Podemos trabalhar com concorrÃªncia usando vÃ¡rios {pcb} processos {df} de uma 
thread ou um Ãºnico processo com vÃ¡rias {tcb} threads {df}.
__L__
Em Python, podemos criar um ou vÃ¡rios processos com os mÃ³dulos: 
__L__
{star} {pur}concurrent.futures.ProcessPoolExecutor{df} (recomendado) __L__
{star} {pc}multiprocessing{df} (de forma manual)
__L__
{bi}Vamos falar de paralelismo e outros tipos de concorrÃªncia ainda neste vÃ­deo.{df}
"""

teory_char_threads = f"""\
{hea}â¯â¯ Threads (o fio da meada){df}
__L__
{hi} Threads {df} representam as sequÃªncias de instruÃ§Ãµes do cÃ³digo compilado 
que a CPU executa. VocÃª pode imaginÃ¡-las como um contexto de execuÃ§Ã£o que pode 
ser pausado e restaurado pelo SO {dii}(explico mais Ã  frente no vÃ­deo){df}.
__L__
As {bi}threads tÃªm acesso aos recursos do processo em que estÃ£o{df}, como memÃ³ria 
e arquivos abertos. Isso as torna mais leves para serem criadas e gerenciadas do 
que processos {dii}(embora ainda exista um pequeno custo){df}.
__L__
Pelo mesmo motivo acima, a comunicaÃ§Ã£o entre {tcb} threads {df} Ã© bem mais fÃ¡cil 
do que a comunicaÃ§Ã£o entre processos {dii}(que sÃ£o isolados, como vimos){df}
__L__
VocÃª pode criar novas threads com os mÃ³dulos: 
__L__
{star} {pur}concurrent.futures.ThreadPoolExecutor{df} (recomendado) __L__
{star} {tc}threading{df} (de forma manual, mas em alto nÃ­vel) __L__
{star} {cya}_thread{df} (de forma manual, mas em baixo nÃ­vel) 
__L__
{bi}Vamos falar dos tipos de concorrÃªncia com threads ainda neste vÃ­deo.{df}
"""

teory_char_threads_graph = f"""\
{hea}â¯â¯ Exemplo em diagrama ASCII {df}
__L__{cya}
  â•­â”€â”€â”€â”€â”€â”€â•®            â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®                      â•­â”€ contÃªiner â”€â•®
  â”‚  SO  â”‚â”€ executa â”€â–¶â”‚  Programas  â”‚â”€ representados por â”€â–¶â”‚  Processos  â”‚â”€â•®
  â•°â”€â”€â”€â”€â”€â”€â•¯            â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯                      â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
  â•­â”€ job â”€â•®                                â•­â”€â”€contextâ”€â”€â•®                   â”‚
  â”‚  CPU  â”‚â—€â”€ que executam sequÃªncias na â—€â”€â”‚  Threads  â”‚â—€â”€â”€â”€â”€ que tÃªm â”€â”€â”€â”€â”€â•¯
  â•°â”€â”€â”€â”€â”€â”€â”€â•¯                                â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

  {dii}Exemplo supercial da execuÃ§Ã£o de um programa.
  Um programa pode criar threads adicionais quando precisa fazer trabalhos 
  de forma concorrente.{df}
"""

teory_diff_python = f"""\
{hea}â¯â¯ O Python Ã© compilado e interpretado ao mesmo tempo{df}
__L__
Em {bi}Python{df}, este processo Ã© {bi}diferente de linguagens compiladas{df}. 
__L__
Quando vocÃª executa um {hiy} cÃ³digo Python {df}, o SO cria um {bi}processo{df} com os recursos 
necessÃ¡rios para o programa. 
__L__
Em seguida, uma {bi}thread{df} Ã© ligada este processo, com o contexto de execuÃ§Ã£o onde 
o cÃ³digo vai rodar.
__L__
Assim que vocÃª pressiona {hi} Enter â {df}, o Python compila seu cÃ³digo para 
{hi} bytecode {df}, que entÃ£o serÃ¡ lido pelo interpretador. 
__L__
O {bi}binÃ¡rio do Python{df} inclui:
__L__
{star} um parser (faz o parsing da Abstract syntax tree da linguagem) __L__
{star} um compilador (compila *.py para bytecode) __L__
{star} o interpretador (Ã© o que vocÃª acha que Ã© o Python) __L__
{star} a API em C (nome auto explicativo)__L__ 
{star} a biblioteca padrÃ£o (que vocÃª usa em todo cÃ³digo)__L__ 
{star} e outras partes que cuidam do ciclo de execuÃ§Ã£o
__L__
Daqui em diante, quem assume Ã© o {hi} C {df} (por isso CPython), a thread e a CPU. 
"""

teory_diff_python_bytecode = f"""\
{hea}â¯â¯ Exemplo do bytecode (mÃ³dulo dis){df}
__L__{yel}
{yel}   â”Œâ”€â”€â”€ arquivo: module.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   {df}
{yel}   â”‚                                                           â”‚   {df} 
{yel}   â”‚ 0. def func() -> None:                                    â”‚   {df}
{yel}   â”‚ 1.   print("Hello world!")                                â”‚{yel}â”€â”€â”{df}   
{yel}   â”‚                                                           â”‚{yel}  â”‚{df}   
{yel}   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜{yel}  â”‚{df}   
{pin}   â”â•â•â• Terminal â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”“{yel}  â”‚{df}   
{pin}   â”ƒ                                                           â”ƒ{yel}â—€â”€â”˜{df}   
{pin}   â”ƒ otavio@local:~$ python module.py                          â”ƒ{pin}â”€â”€â”{df}   
{pin}   â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›{pin}  â”‚{df}   
{cya}   â”Œâ”€â”€â”€ dis.dis(func) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”{pin}  â”‚{df}   
{cya}   â”‚ 4           RESUME                   0                    â”‚{pin}  â”‚{df}   
{cya}   â”‚                                                           â”‚{pin}  â”‚{df}   
{cya}   â”‚ 5           LOAD_GLOBAL              1 (print + NULL)     â”‚{pin}  â”‚{df}   
{cya}   â”‚             LOAD_CONST               0 ('Hello world!')   â”‚{pin}â—€â”€â”˜{df}   
{cya}   â”‚             CALL                     1                    â”‚   {df}
{cya}   â”‚             POP_TOP                                       â”‚   {df}
{cya}   â”‚             LOAD_CONST               1 (None)             â”‚   {df}
{cya}   â”‚             RETURN_VALUE                                  â”‚   {df}
{cya}   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   {df}
{df}
"""

teory_diff_python_graph = f"""\
{hea}â¯â¯ O fluxo do seu cÃ³digo em ASCII {df}
__L__{cya}
  â•­â”€ module.py â”€â”€â•®                 â•­â”€â”€ *.pyc â”€â”€â”€â•®             â•­ python3.14t â•®
  â”‚  seu cÃ³digo  â”‚â”€ compilado p/ â”€â–¶â”‚  bytecode  â”‚â”€ lido por â”€â–¶â”‚   CPython   â”‚â”€â•®
  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯                 â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯             â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
  â•­ Core1 â•®  â•­â”€ TID 321 â”€â•®                     â•­â”€ PID 1234 â”€â•®                 â”‚
  â”‚  CPU  â”‚â—€â”€â”‚  threads  â”‚â—€â”€ com contexto em â”€â”€â”‚  Processo  â”‚â—€â”€â”€ que estÃ¡ em â”€â•¯
  â•°â”€â”€â”€â”€â”€â”€â”€â•¯  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯                     â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

  {dii} Exemplo supercial da execuÃ§Ã£o do Python{df}
__L__ 
"""

teory_chars_summary = f"""\
{hea}â¯â¯ Um breve resumo sobre tudo o que falamos {df}
__L__ 
O Python nÃ£o roda o seu cÃ³digo especificamente, mas {tcb} bytecode {df}.
__L__
O {acb} bytecode {df} Ã© o seu cÃ³digo compilado de uma forma que o Interpretador entenda. 
Isso envolve parsing, compilaÃ§Ã£o e bastante {bi}C{df}.
__L__
Quem gerencia essa comunicaÃ§Ã£o entre {bi}bytecode{df}, {bi}threads{df} e {bi}CPU{df} Ã© o {hi} CPython {df}.
Essa Ã© a implementaÃ§Ã£o mais comum do {bi}interpretador Python{df}, mas existem outras. 
__L__
O {bi}sistema operacional{df} gerencia os {pcb} processos {df} e as {tcb} threads {df} reais.
__L__
VocÃª pode criar mais processos e/ou threads quando precisar de concorrÃªncia. Mas, 
isso terÃ¡ um custo adicional em recursos.
Os mÃ³dulos para isso podem ser: {tcb} threading {df}, {pcb} multiprocessing {df} e {hi} concurrent.futures {df}.
__L__
AlÃ©m disso, Ã© possÃ­vel fazer concorrÃªncia via API do C, com "subinterpreters" (3.14) e 
{hi} asyncio {df} {dii}(vou falar sobre asyncio adiante){df}.
__L__
Agora que conhecemos o Python melhor, podemos falar dos problemas que sÃ£o resolvidos 
com concorrÃªncia.
"""

teory_concurrency_bounds = f"""\
{hea}â¯â¯ Termos e problemas que podemos resolver com concorrÃªncia{df} 
__L__
Os {bi}termos{df} mais comuns relacionados aos problemas que podem ser resolvidos 
com {bi}concorrÃªncia{df} sÃ£o: 
__L__
{star} {bi}I/O-bound{df} __L__
{star} {bi}CPU-bound{df}
__L__
O {hi} bound {df} significa {i}âlimitado porâ{df}, entÃ£o {i}âessa thread Ã© limitada por I/Oâ{df}
ou {i}âessa thread Ã© limitada por CPUâ{df}.
__L__
Existem outros termos, como {hi} GPU-bound {df}, {hi} Memory-bound {df} e outros. 
Mas, Ã© muito mais importante saber o {hi} fator limitante {df} do que o nome do problema.
"""

teory_io_bound = f"""\
{hea}â¯â¯ {hi} I/O-bound {df} e Multitarefas {acb} cooperativa {df} e {tcb} preemptiva {df}
__L__
{bi}I/O{df} quer dizer {bi}Entrada e SaÃ­da{df}, entÃ£o {hi} I/O-bound {df} estÃ¡ relacionado com 
limitaÃ§Ãµes na entrada e saÃ­da de dados.
__L__
Se a velocidade de entrada e saÃ­da Ã© {bi}menor que a velocidade de processamento{df} 
dizemos que a tarefa Ã© {hi} I/O-bound {df}.
__L__
Em outras palavras: enquanto os dados nÃ£o chegam, a CPU nÃ£o tem o que fazer.
__L__
Alguns exemplos de tarefas {hi} I/O-bound {df}:
__L__
{star} Ler e escrever em arquivos (depende do disco) __L__
{star} Aguardar algo da rede (internet, base de dados, APIs e outras) __L__
{star} A funÃ§Ã£o sleep tambÃ©m Ã© I/O-bound
"""

teory_io_bound_graph = f"""\
{hea}â¯â¯ Problema que pode ser resolvido com concorrÃªncia {df}
__L__{cya}
                          â”â”â”â”â”â”â” Python â”â”â”â”â”â”â”“
                          â”ƒ â–ˆ  Seu programa  â–ˆ â”ƒ
                          â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                            I/O     I/O    I/O
                             â”‚       â”Š      â”Š
                             â”‚  aguardando  â”Š
           â”Œâ”€â”€â”€â”€â”€ REQUEST â”€â”€â”€â”˜       â”Š      â””â”„â”„ aguardando â”„â”„â”
           â–¼                         âœ•                       âœ•
  â”â”â”â”â”â” SERVER â”â”â”â”â”â”“     â”Œâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ tarefa â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”     â”Œâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ tarefa â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”
  â”ƒ â–ˆ  PREPARANDO  â–ˆ â”ƒ     â”Š â•³   INTERFACE  â•³ â”Š     â”Š â•³     API      â•³ â”Š
  â”ƒ â–ˆ   RESPONSE   â–ˆ â”ƒ     â”Š â•³    TRAVADA   â•³ â”Š     â”Š â•³ NÃƒO RESPONDE â•³ â”Š
  â”—â”â”â” executando â”â”â”â”›     â””â”ˆâ”ˆâ”ˆ aguardando â”ˆâ”ˆâ”ˆâ”˜     â””â”ˆâ”ˆâ”ˆ aguardando â”ˆâ”ˆâ”ˆâ”˜

  {df}
  {dii}Na ilustraÃ§Ã£o, ao aguardar a resposta de uma requisiÃ§Ã£o para um servidor, 
  nenhuma outra tarefa poderia ser executada sem concorrÃªncia.{df}
"""

teory_python_io_modules = f"""\
{hea}â¯â¯ asyncio e threading sÃ£o soluÃ§Ãµes para I/O-Bound {df}
__L__
Em Python, podemos usar os mÃ³dulos {acb} asyncio {df} e/ou {tcb} threading {df} 
para solucionar problemas I/O-bound. Cada um tem seus pontos fortes e fracos.
__L__ 
Quando digo {tc}threading{df}, estou me referindo ao mÃ³dulo do Python mesmo. Mas, 
na maioria das vezes, uso {pur}concurrent.futures.ThreadPoolExecutor{df}.
__L__ 
Vou usar o nome {tc}threading{df} como qualquer mÃ³dulo Python para threads.
"""

teory_threads = f"""\
{hea}â¯â¯ Threads em mais detalhes {df}
__L__
MÃºltiplas {tcb} Threads {df} podem estar ligadas a um Ãºnico processo e trabalharem 
em conjunto para um objetivo. Isso permite resolver vÃ¡rios tipos de concorrÃªncia. 
__L__
Threads tÃªm benefÃ­cios interessantes, se comparadas Ã  criaÃ§Ã£o de novos processos:
__L__
{star} EstÃ£o ligadas e compartilham recursos com o processo. EntÃ£o sÃ£o mais leves. __L__
{star} Sua comunicaÃ§Ã£o Ã© infinitamente mais fÃ¡cil do que entre processos. __L__
{star} âš ï¸âš ï¸ Conseguem trabalhar em paralelo em mÃºltiplos nÃºcleos da CPU  __L__
{star} TambÃ©m podem trabalhar em multitarefas em um Ãºnico nÃºcleo 
__L__
Apesar de muito mais leves que processos, {bi}threads{df} tambÃ©m tem custo para o sistema. 
Por isso, criar threads {bi}onde nÃ£o Ã© necessÃ¡rio{df} deixa o {hi} cÃ³digo mais lento {df}.
__L__
Os Sistemas operacionais modernos sÃ£o preemptivos. EntÃ£o, fazem troca de contexto entre 
threads periodicamente. Isso tem custo, jÃ¡ que Ã© necessÃ¡rio salvar e restaurar o 
contexto a cada troca {dii}(em inglÃªs, context switch){df}.
__L__
Esse tipo de concorrÃªncia Ã© chamado de {tcb} Multitarefa Preemptiva {df} e permite que 
um sistema execute 10 programas, em 2 nÃºcleos da CPU, como se executassem ao mesmo tempo.
"""

teory_asyncio = f"""\
{hea}â¯â¯ asyncio em mais detalhes {df}
__L__
{acb} asyncio {df}, tem o {bi}comportamento cooperativo{df}, onde o desenvolvedor escolhe 
o momento para ceder o controle. Esse tipo de comportamento Ã© chamado de 
{acb} Multitarefa Cooperativa {df} e alcanÃ§a concorrÃªncia de forma inteligente.
__L__
Ao invÃ©s de mÃºltiplas {bi}threads{df} (como vimos), {acb} asyncio {df}, roda em
uma {bi}Ãºnica thread{df}. Da mesma forma que um script Python sem concorrÃªncia.
__L__
A concorrÃªncia acontece entre um {acb} Event Loop {df} (loop de eventos) e {bi}corrotinas{df}. 
__L__
Para quem conhece Python, corrotinas sÃ£o funÃ§Ãµes criadas com {hi} async def {df}. 
Elas podem ceder o controle de volta para o Event Loop quando estÃ£o ocupadas.
__L__
Isso permite que, {bi}ao realizar operaÃ§Ãµes I/O-bound{df}, uma corrotina ceda o 
controle para que o loop {bi}execute outra corrotina{df} que estiver pronta.
__L__
Isso Ã© inteligente por nÃ£o gerar overhead de criaÃ§Ã£o de threads ou processos, nem 
trocas de contexto. {dii}(ocorre troca de contexto da thread Ãºnica apenas){df}.
"""

teory_multitask_summary = f"""\
{hea}â¯â¯ asyncio e threads: um resumo sobre o que vimos{df}
__L__
{ac}asyncio{df} faz {bi}Multitarefa Cooperativa{df} e usa uma Ãºnica thread. 
__L__
O prÃ³prio desenvolvedor decide o momento para pausar e/ou continuar.
__L__
Seu benefÃ­cio tambÃ©m pode ser um problema. Se algo bloqueante for executado no 
{ac}Event Loop{df}, isso vai bloquear as outras corrotinas que estiverem prontas. 
__L__
Por isso, ou executamos tudo assÃ­ncrono (nÃ£o bloqueante), ou movemos tudo o que for 
bloqueante para outra thread. {dii}(isso Ã© bem simples de fazer){df}.
__L__
{tc}Threads{df} fazem {bi}Multitarefa Preemptiva{df} a nÃ­vel de sistema operacional.
Ele decide quando pausar e/ou continuar usando seus critÃ©rios. O dev nÃ£o decide isso.
__L__
Criar muitas {tcb} threads {df} mais a troca de contexto pode fazer uma tarefa ficar 
ainda mais lenta do que rodar o cÃ³digo sem concorrÃªncia. {dii}(precisamos testar){df}.
__L__
Outro fato que ainda nÃ£o mencionei, mas faz {tc} TODA A DIFERENÃ‡A {df}, Ã© o GIL...
Vamos falar sobre isso na prÃ³xima tela ğŸ”’...
"""

teory_the_gil = f"""\
{hea}â¯â¯ Threads sÃ£o afetadas pelo {hi} GIL {df}{dii} (Global Interpreter Lock){df}.
__L__
{tcb} Nota: {df} {bi}desconsidere o GIL se vocÃª o desativar. (Python 3.14){df}
__L__
GIL Ã© o Lock global do Python que {bi}nÃ£o permite que threads do mesmo processo rodem 
ao mesmo tempo{df}.
__L__
Ele Ã© um mecanismo de proteÃ§Ã£o do Python para evitar {dii}Race Conditions{df}.
__L__
Ironicamente, remover um lock global como o GIL, envolve a criaÃ§Ã£o de outros locks 
de menor escopo em locais especÃ­ficos. {bi}Isso deixarÃ¡ o Python mais lento.{df}
__L__
Tudo isso significa que, mesmo com vÃ¡rias threads, seu cÃ³digo pode se comportar 
como um cÃ³digo sem concorrÃªncia, mas com o peso das threads {dii}(que vimos antes){df}.
__L__
A boa notÃ­cia que Ã© a soluÃ§Ã£o veio no {hi} Python 3.14 {df} {dii}(jÃ¡ falo sobre isso){df}.
"""

teory_gil_analogy_one = f"""\
{hea}â¯â¯ Uma analogia para vocÃª entender o GIL (1/2){df}
__L__
Imagine as {hi} threads {df} como pessoas em uma festa.
__L__
A {hi} CPU {df} Ã© o banheiro do local. 
__L__
Este banheiro Ã© enorme e tem muitas cabines livres para diversas pessoas.
__L__
SÃ³ que a polÃ­tica da festa diz:
__L__
{hiy} PROIBIDO MAIS DE UMA PESSOA NO BANHEIRO {df}
__L__
Para garantir que a polÃ­tica serÃ¡ seguida, eles forÃ§am que as {bi}pessoas levem 
a chave quando precisarem ir ao banheiro. E mais, tambÃ©m se tranquem por dentro{df}.
__L__
Uma pessoa sÃ³ {hi} USA UMA CABIBE {df} e termina o serviÃ§o. 
__L__
EntÃ£o, ela destranca a porta e pode sair.
__L__
Ao sair, ela olha para a fila e entrega a chave para a prÃ³xima pessoa (thread)
que estiver esperando para entrar.
"""

teory_gil_analogy_two = f"""\
{hea}â¯â¯ Uma analogia para vocÃª entender o GIL (2/2) {df}
__L__
O processo se repete para cada pessoa que entrar no banheiro.
__L__
Essa ğŸ”‘ chave da analogia, tem o comportamento do ğŸ”’ GIL do Python. Percebe o 
desperdÃ­cio de CPU, quando apenas uma thread pode usÃ¡-la por vez?
__L__
O Python {hi} libera o GIL {df} ao iniciar uma {bi}tarefa que vai aguardar por I/O{df}. 
EntÃ£o, ele nÃ£o seria um problema se usarmos {bi}threads em tarefas I/O-bound{df}.
OperaÃ§Ãµes de I/O-bound, geralmente ocorrem em tempo sobreposto.
__L__
{bi}No paralelismo, o GIL SERIA UM PROBLEMA REAL{df} {dii}(adiante eu explico){df}.
__L__
{bi}Como tudo o que descrevi nesse texto todo, vocÃª precisa fazer testes.{df} 
"""

teory_a_new_python = f"""\
{hea}â¯â¯ A NOTÃCIA QUE MUDA TUDO PARA AS THREADS {df}
__L__
O {bi}Python 3.14 permite desativar o GIL{df} (simples e direto assim).
__L__
SÃ³ essa Ãºnica frase jÃ¡ desfaz muito do que falei antes (teoricamente).
__L__
Essa versÃ£o do Python foi lanÃ§ada hÃ¡ poucas semanas e fiz poucos testes sem o GIL.
Tive resultados promissores nesses testes que fiz, mas ainda Ã© cedo para opinar.
__L__
NÃ£o fosse essa notÃ­cia, {bi}neste ponto eu estaria te explicando o motivo de vocÃª 
ser obrigado a criar vÃ¡rios processos{df} ao fazer paralelismo em Python.
__L__
Por falar em nisso, vamos ver o que Ã© {hi} paralelismo e tarefas CPU-bound {df}.
"""

teory_cpu_bound = f"""\
{hea}â¯â¯ CPU-bound e Paralelismo{df}
__L__
Se {bi}I/O-bound{df} Ã© um tempo de espera inÃºtil que pode ser gasto com outra tarefa,
{hi} CPU-bound {df} sÃ£o tarefas que envolvem gargalo de CPU ou de nÃºcleo de CPU.
__L__
Exemplos comuns de tarefas CPU-bound envolvem:
__L__
{star} EdiÃ§Ã£o e renderizaÃ§Ã£o de vÃ­deos __L__
{star} Machine learning __L__
{star} SimulaÃ§Ãµes e previsÃµes complexas __L__
{star} CÃ¡lculos cientÃ­ficos __L__
{star} Criptografia 
__L__
Temos duas formas possÃ­veis para solucionar este problema: 
__L__
{star} MÃºltiplos {pcb} Processos single thread {df} __L__
{star} MÃºltiplas {tcb} Threads do mesmo processo {df} 
__L__
As duas soluÃ§Ãµes envolvem mÃºltiplos nÃºcleos de CPU.
"""

teory_cpu_bound_graph = f"""\
{hea}â¯â¯ Exemplo de gargalo em um Ãºnico nÃºcleo{df}
__L__ {cya}
                          â”â”â”â”â”â”â” Python â”â”â”â”â”â”â”“
                          â”ƒ â–ˆ  Seu programa  â–ˆ â”ƒ
                          â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                            CPU     CPU    CPU
                             â”‚       â”Š      â”Š
                             â”‚    ocioso    â”Š
           â”Œâ”€â”€ executando  â”€â”€â”˜       â”Š      â””â”„â”„â”„â”„ ocioso â”„â”„â”„â”„â”
           â–¼                         âœ•                       âœ•
  â”â”â”â”â” Uso 100% â”â”â”â”â”“     â”Œâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ Uso 0% â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”     â”Œâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ Uso 0% â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”
  â”ƒ â–ˆ Renderizando â–ˆ â”ƒ     â”Š     .........    â”Š     â”Š     .........    â”Š
  â”ƒ â–ˆ    VÃ­deo     â–ˆ â”ƒ     â”Š                  â”Š     â”Š                  â”Š
  â”—â”â”â” no limite â”â”â”â”â”›     â””â”ˆâ”ˆâ”ˆâ”ˆâ”ˆ livre â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”˜     â””â”ˆâ”ˆâ”ˆâ”ˆâ”ˆ livre â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”˜
  {df}
  {dii}No exemplo acima, ao executar uma tarefa em um Ãºnico nÃºcleo da CPU, o problema 
  fica claro.{df}
"""

teory_cpu_bound_hardware = f"""\
{hea}â¯â¯ OperaÃ§Ãµes CPU-bound sÃ£o limitadas pelo hardware{df}
__L__
O tempo de espera {hi} nÃ£o Ã© inÃºtil {df}. Pode ocorrer algum desperdÃ­cio, mas por 
estarem operando em carga mÃ¡xima ou por erro do programador. 
O fato Ã© que o limite Ã© a prÃ³pria CPU (ou nÃºcleo da CPU).
__L__
SÃ³ Ã© possÃ­vel resolver este problema se a CPU tem mais nÃºcleos disponÃ­veis. Do 
contrÃ¡rio, sÃ³ um upgrade mesmo.
__L__
O tipo de concorrÃªncia onde usa-se vÃ¡rios nÃºcleos da CPU ao mesmo tempo Ã© chamado 
de {hi} paralelismo {df}.

"""
teory_io_vs_cpu_bound = f"""\
{hea}â¯â¯ Tempo sobreposto e Paralelismo (I/O-bound e CPU-bound){df}
__L__
No {tcb} tempo sobreposto {df} das multitarefas Cooperativa e Preemptiva, uma thread 
pode executar enquanto outra estÃ¡ ocupada. Pode ocorrer no mesmo nÃºcleo da CPU ou 
atÃ© em uma mesma thread. Os tempos se sobrepÃµe. {dii} (asyncio e threads){df}.
__L__
No {pcb} paralelismo {df}, uma thread pode ser executada ao mesmo tempo que a 
outra em nÃºcleos diferentes da CPU. Ocorre com vÃ¡rias threads do mesmo processo 
ou vÃ¡rios processos single thread.

"""

teory_io_vs_cpu_bound_graph_a = f"""\
{hea}â¯â¯ Multitarefa preemptiva e Paralelismo com threads{df}
__L__
{tcb} Na esquerda: {df} {dii}CPU single core{df} {bi}nÃ£o faz paralelismo{df}, faz {tc}{bi}PREEMPÃ‡ÃƒO{df}.
{pcb} Na direita: {df} {dii}CPU multi core{df} executa threads {pc}{bi}AO MESMO TEMPO{df} em {dii}cores{df} diferentes{df}.
__L__
{tc}  Multitarefa Preemptiva              {df}{pc}   Paralelismo (âš ï¸ GIL)
{tc}  (mesmo proc. e core, vÃ¡rias threads){df}{pc}   (mesmo proc., vÃ¡rias threads e cores)
{tc}  
{tc}  Core 1 / Proc. 1 / Thread 1         {df}{pc}    Core 1 / Proc. 1 / Thread 1
{tc}  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆ         {df}{pc}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
{tc}  Core 1 / Proc. 1 / Thread 2         {df}{pc}    Core 2 / Proc. 1 / Thread 2
{tc}  â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆ             {df}{pc}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
{tc}  Core 1 / Proc. 1 / Thread 3         {df}{pc}    Core 3 / Proc. 1 / Thread 3
{tc}  â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆ           {df}{pc}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
{tc}  Core 1 / Proc. 1 / Thread 4         {df}{pc}    Core 4 / Proc. 1 / Thread 4
{tc}  â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆ                 {df}{pc}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
{tc}              â–¼                       {df}{pc}                â–¼
{tc}  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         {df}{pc}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
{tc}  PercepÃ§Ã£o do usuÃ¡rio: tudo          {df}{pc}    Tudo realmente aconteceu ao
{tc}  aconteceu ao mesmo tempo            {df}{pc}    mesmo tempo.
{df}
"""

teory_io_vs_cpu_bound_graph_b = f"""\
{hea}â¯â¯ Multitarefa cooperativa e Paralelismo com mÃºltiplos processos{df}
__L__
{acb} Na esquerda: {df} {dii}mesmo processo, core e thread{df}, cÃ³digo {ac}{bi}assÃ­ncrono{df} {dii}(concorrente){df}.
{pcb} Na direita: {df} {dii}mÃºltiplos processos single-thread{df} executando {pc}{bi}AO MESMO TEMPO{df} {dii}(paralelismo){df}.
__L__
{ac}  Multitarefa Cooperativa             {df}{pc}   Paralelismo (âœ… GIL)
{ac}  (mesmo proc., core e thread)        {df}{pc}   (vÃ¡rios procs. single thread)
{ac}  
{ac}  Core 1 / Proc. 1 / Thread 1         {df}{pc}    Core 1 / Proc. 1 / Main Thread
{ac}  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆ         {df}{pc}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
{ac}  Core 1 / Proc. 1 / Thread 1         {df}{pc}    Core 2 / Proc. 2 / Main Thread
{ac}  â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆ             {df}{pc}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
{ac}  Core 1 / Proc. 1 / Thread 1         {df}{pc}    Core 3 / Proc. 3 / Main Thread
{ac}  â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆ           {df}{pc}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
{ac}  Core 1 / Proc. 1 / Thread 1         {df}{pc}    Core 4 / Proc. 4 / Main Thread
{ac}  â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆ                 {df}{pc}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
{ac}              â–¼                       {df}{pc}                â–¼
{ac}  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         {df}{pc}    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
{ac}  PercepÃ§Ã£o do usuÃ¡rio: tudo          {df}{pc}    Tudo realmente aconteceu ao
{ac}  aconteceu ao mesmo tempo            {df}{pc}    mesmo tempo.
{df}
"""
teory_io_vs_cpu_bound_graph_a = f"{cya}{teory_io_vs_cpu_bound_graph_a}"

teory_parallelism = f"""\
{hea}â¯â¯ GIL e o paralelismo{df}
__L__
Nessa altura do vÃ­deo, eu devo ter falado do GIL e paralelismo umas 5 vezes. 
__L__
Mas para oficializar: {bi}nÃ£o tem como fazer paralelismo com threads do mesmo 
processo com o GIL ativo.{df}
__L__
A {bi}Ãºnica opÃ§Ã£o{df} que eu tinha para {bi}paralelismo{df} no Python em versÃµes 
anteriores era {hi} criando vÃ¡rios processos {df} ou usando outra implementaÃ§Ã£o.
__L__
Se eu manter a {bi}main thread do processo{df} e criar vÃ¡rios processos 
o {hi} GIL nÃ£o Ã© um problema {df}, jÃ¡ que processos sÃ£o isolados.
__L__
Isso funcionou assim por anos, mas o {bi}custo Ã© enorme{df}.
"""

teory_proccess_pain = f"""\
{hea}â¯â¯ Qual o problema de vÃ¡rios processos?{df}
__L__
Vamos lembrar algumas coisas que falei nesse mesmo vÃ­deo:
__L__
{star} Um {bi}processo Ã© isolado{df} de outros processos do sistema. Tem como fazer
  comunicaÃ§Ã£o entre processos (IPC), {bi}mas envolve muita complexidade{df}. A sincronizaÃ§Ã£o
  de dados em concorrÃªncia jÃ¡ Ã© complexa por si sÃ³. Se a comunicaÃ§Ã£o Ã© entre processos, 
  {bi}nÃ³s estamos tratando de um dos assuntos mais complexo da programaÃ§Ã£o{df}. __L__
{star} Um {bi}processo nÃ£o Ã© algo trivial para o sistema{df}. Se custa caro subir um Ãºnico
  processo, imagine vÃ¡rios. __L__
{star} Se o Python vai rodar em outro processo, significa que eu {bi}preciso de outro
  interpretador{df} no outro processo. Ã‰ quase o mesmo que eu recarregar a o
  programa do zero (outro custo). __L__
{star} Os {bi}dados precisam ser serializados{df}. NÃ£o Ã© qualquer tipo de dado que pode
  usado com processos.
"""
teory_new_interpreters = f"""\
{hea}â¯â¯ Subinterpreters (concurrent interpreters){df}
__L__
Vou fazer uma menÃ§Ã£o honrosa aos {hi} Concurrent Interpreters {df} do Python 3.14.
__L__
Creio que esse nÃ£o Ã© o nome deles, mas estou usando este nome por estar usando 
{pur}concurrent.futures.InterpreterPoolExecutor{df} para subir vÃ¡rios interpretadores 
ao mesmo do Python de forma concorrente. 
__L__
Eles {hi} tambÃ©m fazem paralelismo {df} no Python e sÃ£o bem promissores.
__L__
A documentaÃ§Ã£o fala sobre eles como tendo todos os benefÃ­cios das threads mas com 
comportamento de processos. 
__L__
Tudo o que te expliquei sobre os processos, funciona de forma similar nos interpretadores. 
__L__
Eles sÃ£o isolados e seguros como os processos. O comportamento de thread Ã© pelo fato 
de {hi} eles rodarem no mesmo processo {df}. 
__L__
O Ãºnico problema que percebi com os interpretadores atÃ© agora, Ã© que eles {bi}ainda 
consomem muita memÃ³ria{df}. Mas a documentaÃ§Ã£o jÃ¡ indicou que estÃ£o trabalhando 
ativamente neste recurso.
__L__
Quem sabe eu consiga encaixar eles nesses vÃ­deos sobre concorrÃªncia? Me ajuda aÃ­!
"""
teory_end = f"""\
{hea}â¯â¯ E finalmente chegamos ao fim...{df}
__L__
Agora, terminando esse texto enorme que fui perceber. Escrevi um artigo inteiro 
sÃ³ em comentÃ¡rios do Python. Inicialmente, achei que terÃ­amos cÃ³digo ğŸ¤£.
__L__
Quando vocÃª for assistir ao vÃ­deo, isso jÃ¡ deverÃ¡ estar bem diferente. Pretendo 
usar asyncio para fazer uma espÃ©cie de slides e depois te mostrar o cÃ³digo.
__L__
NÃ£o sei se vai dar certo, mas vou manter isso aqui para a posteridade.
__L__
No mais... Obrigado por chegar atÃ© o final {red}s2{df}.
__L__
Fim...
"""

concurrency_teory: Sequence[str] = [
    teory_intro,
    teory_non_concurrent_python,
    teory_char_process,
    teory_char_threads,
    teory_char_threads_graph,
    teory_diff_python,
    teory_diff_python_bytecode,
    teory_diff_python_graph,
    teory_chars_summary,
    teory_concurrency_bounds,
    teory_io_bound,
    teory_io_bound_graph,
    teory_python_io_modules,
    teory_threads,
    teory_asyncio,
    teory_multitask_summary,
    teory_the_gil,
    teory_gil_analogy_one,
    teory_gil_analogy_two,
    teory_a_new_python,
    teory_cpu_bound,
    teory_cpu_bound_graph,
    teory_cpu_bound_hardware,
    teory_io_vs_cpu_bound,
    teory_io_vs_cpu_bound_graph_a,
    teory_io_vs_cpu_bound_graph_b,
    teory_parallelism,
    teory_proccess_pain,
    teory_new_interpreters,
    teory_end,
]
